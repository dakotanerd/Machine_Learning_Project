{"prompt": "Analyze this Unknown code for vulnerabilities:\n#include <iostream>\n#include <cstring>\nusing namespace std;\nint main() {\n    char buf[5];\n    strcpy(buf, \"Overflow\"); // buffer overflow\n    cout << buf << endl;\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n#include <iostream>\n#include <vector>\nusing namespace std;\nint main() {\n    vector<int>* v = new vector<int>(5);\n    delete v;\n    cout << v->at(0) << endl; // use-after-delete\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n#include <iostream>\nusing namespace std;\nint main() {\n    string token = \"secret123\"; // hardcoded token\n    cout << \"Token: \" << token << endl;\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n#include <iostream>\nusing namespace std;\nint main() {\n    string token = \"secret123\"; // hardcoded token\n    cout << \"Token: \" << token << endl;\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n#include <iostream>\n#include <string>\nusing namespace std;\nint main() {\n    string s;\n    cin >> s;\n    cout << s << endl; // no sanitization\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C++ code for vulnerabilities:\n#include <iostream>\n#include <vector>\n#include <cstring>\n#include <cstdlib>\nusing namespace std;\n\nvoid run() {\n    char buf[5];\n    strcpy(buf, \"Overflow\"); // buffer overflow\n\n    vector<int>* v = new vector<int>(5);\n    delete v;\n    cout << v->at(0) << endl; // use-after-delete\n\n    string token = \"secret123\"; // hardcoded token\n    cout << \"Token: \" << token << endl;\n\n    system(\"ls -la\"); // command execution\n}\n\nint main() {\n    run();\n    return 0;\n}\n\n", "completion": "strcpy usage (possible buffer overflow) | system() usage (command injection risk)"}
{"prompt": "Analyze this C++ code for vulnerabilities:\n#include <iostream>\n#include <vector>\nusing namespace std;\n\nint add(int a, int b) { return a + b; }\n\nint main() {\n    vector<int> nums = {1,2,3};\n    for(int n: nums) cout << n << \" \";\n    cout << endl;\n    cout << \"Sum: \" << add(5,7) << endl;\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdlib.h>\nint main() {\n    int *ptr = malloc(sizeof(int)*5);\n    free(ptr);\n    *ptr = 10; // use-after-free\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\nint main() {\n    int password = 12345; // hardcoded password\n    printf(\"Password: %d\\n\", password);\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\nint main() {\n    char buf[20];\n    gets(buf); // unsafe, can overflow\n    printf(\"Hello %s\\n\", buf);\n    return 0;\n}\n\n", "completion": "gets() usage (unsafe)"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\nint main() {\n    char buf[20];\n    printf(buf); // format string attack possible\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\n#include <string.h>\n#include <stdlib.h>\n\nint main() {\n    char buf[10];\n    strcpy(buf, \"This string is way too long!\"); // Buffer overflow\n\n    char *ptr = malloc(sizeof(int)*5);\n    free(ptr);\n    ptr[0] = 42; // Use-after-free\n\n    int password = 12345; // Hardcoded credential\n    printf(\"Password: %d\\n\", password);\n\n    char input[20];\n    gets(input); // unsafe input\n    printf(\"Hello %s\\n\", input);\n\n    return 0;\n}\n\n", "completion": "strcpy usage (possible buffer overflow) | gets() usage (unsafe)"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\n#include <string.h>\n\nint sum(int a, int b) {\n    return a + b;\n}\n\nint main() {\n    int total = sum(5, 10);\n    printf(\"Total: %d\\n\", total);\n    return 0;\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this C code for vulnerabilities:\n#include <stdio.h>\n#include <string.h>\nint main() {\n    char buf[10];\n    strcpy(buf, \"This string is too long!\"); // buffer overflow\n    printf(\"%s\\n\", buf);\n    return 0;\n}\n\n", "completion": "strcpy usage (possible buffer overflow)"}
{"prompt": "Analyze this Java code for vulnerabilities:\nclass Java2 {\n    public static void main(String[] args) {\n        String password = \"secret\"; // hardcoded\n        System.out.println(password);\n    }\n}\n\n", "completion": "Hardcoded credentials"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.io.*;\nclass Java3 {\n    public static void main(String[] args) throws Exception {\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"data.ser\"));\n        Object obj = ois.readObject(); // unsafe deserialization\n    }\n}\n\n", "completion": "Potential unsafe deserialization"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.sql.*;\nclass Java4 {\n    public static void main(String[] args) throws Exception {\n        String userInput = \"1 OR 1=1\";\n        String query = \"SELECT * FROM users WHERE id=\" + userInput;\n        System.out.println(query); // potential SQL injection\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\nclass Java5 {\n    public static void main(String[] args) {\n        String token = \"abc123\";\n        System.out.println(token);\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.io.*;\nimport java.util.*;\nimport java.sql.*;\n\nclass User {\n    int id;\n    String username;\n    String password; // Vulnerable: storing passwords in plaintext\n\n    User(int id, String username, String password) {\n        this.id = id;\n        this.username = username;\n        this.password = password;\n    }\n\n    void printUser() {\n        System.out.println(\"ID: \" + id + \", Username: \" + username);\n    }\n}\n\npublic class VulnTest {\n\n    private static List<User> users = new ArrayList<>();\n\n    public static void main(String[] args) {\n        addSampleUsers();\n\n        // Vulnerability 1: SQL Injection\n        try {\n            BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\n            System.out.print(\"Enter username to search: \");\n            String input = reader.readLine();\n            \n            String query = \"SELECT * FROM users WHERE username = '\" + input + \"';\";\n            System.out.println(\"Executing query: \" + query);\n            // Pretend execution\n            // Statement stmt = connection.createStatement();\n            // ResultSet rs = stmt.executeQuery(query);\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n        // Vulnerability 2: Unsafe file handling\n        try {\n            BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\n            System.out.print(\"Enter filename to read: \");\n            String filename = reader.readLine();\n\n            File file = new File(filename); // Unsafe: no path validation\n            BufferedReader fileReader = new BufferedReader(new FileReader(file));\n            String line;\n            while ((line = fileReader.readLine()) != null) {\n                System.out.println(line);\n            }\n            fileReader.close();\n\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n        // Print all users\n        for (User u : users) {\n            u.printUser();\n        }\n    }\n\n    static void addSampleUsers() {\n        users.add(new User(1, \"Alice\", \"password123\"));\n        users.add(new User(2, \"Bob\", \"secret!\"));\n        users.add(new User(3, \"Charlie\", \"qwerty\"));\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.io.*;\n\nclass JavaMulti1 {\n    public static void main(String[] args) throws Exception {\n        Runtime.getRuntime().exec(\"ls -la\"); // command execution\n\n        String password = \"secret\"; // hardcoded\n        System.out.println(password);\n\n        ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"data.ser\"));\n        Object obj = ois.readObject(); // unsafe deserialization\n\n        String userInput = \"1 OR 1=1\";\n        String query = \"SELECT * FROM users WHERE id=\" + userInput; // SQL injection\n        System.out.println(query);\n    }\n}\n\n", "completion": "Runtime.exec usage (command injection risk) | Potential unsafe deserialization | Hardcoded credentials"}
{"prompt": "Analyze this Java code for vulnerabilities:\nclass JavaMulti2 {\n    public static int add(int a, int b) {\n        return a + b;\n    }\n\n    public static void main(String[] args) {\n        int total = add(5, 10);\n        System.out.println(\"Total: \" + total);\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\npublic class Example1 {\n    public static void main(String[] args) {\n        String input = args[0];\n        System.out.println(input);\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.sql.*;\nimport java.io.*;\n\npublic class SQLTest1 {\n\n    public static void main(String[] args) throws Exception {\n        Connection conn = DriverManager.getConnection(\"jdbc:mysql://localhost/testdb\",\"user\",\"pass\");\n\n        BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\n        System.out.print(\"Enter username: \");\n        String userInput = reader.readLine();\n\n        // Vulnerable SQL injection\n        String query = \"SELECT * FROM users WHERE username = '\" + userInput + \"';\";\n        Statement stmt = conn.createStatement();\n        ResultSet rs = stmt.executeQuery(query);\n\n        while(rs.next()) {\n            System.out.println(\"Found: \" + rs.getString(\"username\"));\n        }\n        rs.close();\n        stmt.close();\n        conn.close();\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Java code for vulnerabilities:\nimport java.sql.*;\nimport java.util.Scanner;\n\npublic class SQLTest2 {\n\n    public static void main(String[] args) throws SQLException {\n        Connection conn = DriverManager.getConnection(\"jdbc:mysql://localhost/db\",\"user\",\"pass\");\n        Scanner scanner = new Scanner(System.in);\n\n        System.out.print(\"Search by email: \");\n        String email = scanner.nextLine();\n\n        System.out.print(\"Search by role: \");\n        String role = scanner.nextLine();\n\n        // Two concatenated SQL inputs\n        String sql = \"SELECT * FROM accounts WHERE email='\" + email + \"' AND role='\" + role + \"';\";\n        Statement s = conn.createStatement();\n        ResultSet r = s.executeQuery(sql);\n\n        while(r.next()) {\n            System.out.println(r.getString(\"email\") + \" - \" + r.getString(\"role\"));\n        }\n\n        r.close();\n        s.close();\n        conn.close();\n    }\n}\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this PHP code for vulnerabilities:\n<?php\n// Complex PHP file: login system + SQL injection example\n\nclass User {\n    public $id;\n    public $username;\n    public $password; // storing plaintext for demonstration\n\n    function __construct($id, $username, $password) {\n        $this->id = $id;\n        $this->username = $username;\n        $this->password = $password;\n    }\n}\n\nclass Database {\n    private $conn;\n    function __construct($host, $user, $pass, $db) {\n        $this->conn = new mysqli($host, $user, $pass, $db);\n        if ($this->conn->connect_error) die(\"Connection failed: \".$this->conn->connect_error);\n    }\n    function query($sql) {\n        return $this->conn->query($sql);\n    }\n}\n\n$users = [];\n$users[] = new User(1, \"Alice\", \"password123\");\n$users[] = new User(2, \"Bob\", \"secret!\");\n$users[] = new User(3, \"Charlie\", \"qwerty\");\n\n$db = new Database(\"localhost\",\"root\",\"\",\"mydb\");\n\nif(isset($_POST['username']) && isset($_POST['password'])){\n    $u = $_POST['username'];\n    $p = $_POST['password'];\n    // Vulnerable SQL query\n    $sql = \"SELECT * FROM users WHERE username='$u' AND password='$p'\";\n    $result = $db->query($sql);\n    if($result->num_rows > 0){\n        echo \"Login successful!\";\n    } else {\n        echo \"Login failed!\";\n    }\n}\n\n// Large function to fill dataset\nfunction processData($arr) {\n    $res = [];\n    foreach($arr as $k => $v){\n        $v = trim($v);\n        if(is_numeric($v)){\n            $res[$k] = $v * 2;\n        } else {\n            $res[$k] = strtoupper($v);\n        }\n    }\n    return $res;\n}\n\nfor($i=0;$i<100;$i++){\n    $data = [];\n    for($j=0;$j<50;$j++){\n        $data[] = md5(rand());\n    }\n    $processed = processData($data);\n}\n?>\n\n", "completion": "Hardcoded credentials"}
{"prompt": "Analyze this PHP code for vulnerabilities:\n<?php\n// File upload handler with potential path traversal\n\n$uploadDir = \"uploads/\";\n\nif(isset($_FILES['file'])){\n    $filename = $_FILES['file']['name'];\n    $target = $uploadDir . $filename; // unsafe concatenation\n    move_uploaded_file($_FILES['file']['tmp_name'], $target);\n}\n\nclass FileHandler {\n    public static function readFile($path){\n        if(file_exists($path)){\n            $content = file_get_contents($path);\n            return $content;\n        }\n        return null;\n    }\n}\n\nfor($i=0;$i<200;$i++){\n    $filePath = \"logs/log_\".$i.\".txt\";\n    $content = \"Log entry #\".$i.\"\\n\";\n    file_put_contents($filePath, $content, FILE_APPEND);\n}\n\nfunction scanDirectory($dir){\n    $files = [];\n    foreach(scandir($dir) as $f){\n        if($f === '.' || $f === '..') continue;\n        $full = $dir.\"/\".$f;\n        if(is_dir($full)){\n            $files = array_merge($files, scanDirectory($full));\n        } else {\n            $files[] = $full;\n        }\n    }\n    return $files;\n}\n?>\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this PHP code for vulnerabilities:\n<?php\n// Session management + cookie handling\n\nsession_start();\nif(!isset($_SESSION['user'])){\n    $_SESSION['user'] = \"guest\";\n}\n\nif(isset($_COOKIE['prefs'])){\n    $prefs = unserialize($_COOKIE['prefs']); // unsafe deserialization\n} else {\n    $prefs = ['theme'=>'dark', 'lang'=>'en'];\n}\n\nclass Auth {\n    public static function checkAccess($role){\n        if(!isset($_SESSION['role'])) $_SESSION['role']=\"guest\";\n        return $_SESSION['role'] === $role;\n    }\n}\n\nfunction complexLoop(){\n    $arr = [];\n    for($i=0;$i<500;$i++){\n        $arr[$i] = md5(rand());\n        for($j=0;$j<10;$j++){\n            $arr[$i] .= chr(rand(65,90));\n        }\n    }\n    return $arr;\n}\n\n$data = complexLoop();\nforeach($data as $d){\n    echo substr($d,0,10).\"\\n\";\n}\n?>\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this PHP code for vulnerabilities:\n<?php\n// Arrays, JSON, serialization\n\nclass Product {\n    public $id;\n    public $name;\n    public $price;\n\n    function __construct($id, $name, $price){\n        $this->id = $id;\n        $this->name = $name;\n        $this->price = $price;\n    }\n}\n\n$products = [];\nfor($i=0;$i<100;$i++){\n    $products[] = new Product($i, \"Product\".$i, rand(10,1000)/10);\n}\n\n$json = json_encode($products);\n\n$decoded = json_decode($json, true);\n\n$serialized = serialize($decoded);\n$unserialized = unserialize($serialized); // can be unsafe with untrusted input\n\nfunction transformProducts($arr){\n    $res = [];\n    foreach($arr as $p){\n        $res[] = ['name'=>strtoupper($p['name']), 'price'=>$p['price']*1.1];\n    }\n    return $res;\n}\n\n$final = transformProducts($unserialized);\n?>\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this PHP code for vulnerabilities:\n<?php\n// API request handling, cURL, string processing\n\nfunction callAPI($url, $data){\n    $ch = curl_init($url);\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, http_build_query($data));\n    $response = curl_exec($ch);\n    curl_close($ch);\n    return $response;\n}\n\nfor($i=0;$i<50;$i++){\n    $data = ['user'=>\"user$i\", 'action'=>\"ping\"];\n    $resp = callAPI(\"https://example.com/api\", $data);\n}\n\nfunction complexStringOps($input){\n    $res = [];\n    $words = explode(\" \", $input);\n    foreach($words as $w){\n        $res[] = strrev($w).strtoupper($w);\n    }\n    return implode(\"-\", $res);\n}\n\n$text = str_repeat(\"lorem ipsum dolor sit amet \", 100);\n$processed = complexStringOps($text);\n?>\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Python code for vulnerabilities:\ncode = \"print('Hello')\"\nexec(code)  # arbitrary code execution\n\n", "completion": "Unsafe exec usage"}
{"prompt": "Analyze this Python code for vulnerabilities:\npassword = \"123456\"\nprint(f\"Password is {password}\")\n\n", "completion": "Hardcoded credentials"}
{"prompt": "Analyze this Python code for vulnerabilities:\nimport pickle\ndata = pickle.loads(b\"cos\\nsystem\\n(S'ls'\\ntR.\")  # unsafe deserialization\n\n", "completion": "Unsafe pickle deserialization"}
{"prompt": "Analyze this Python code for vulnerabilities:\nimport requests\nrequests.get(\"https://example.com\", verify=False)  # disables SSL verification\n\n", "completion": "Insecure requests verify=False"}
{"prompt": "Analyze this Python code for vulnerabilities:\n# Vulnerable code example\ndef unsafe(input):\n    eval(input)  # unsafe\n\n", "completion": "Unsafe eval usage"}
{"prompt": "Analyze this Python code for vulnerabilities:\nimport pickle\nimport subprocess\n\nuser_code = input(\"Enter code: \")\neval(user_code)  # unsafe eval\n\ncode = \"print('hello')\"\nexec(code)  # unsafe exec\n\npassword = \"123456\"  # hardcoded password\nprint(f\"Password is {password}\")\n\ndata = pickle.loads(b\"cos\\nsystem\\n(S'ls'\\ntR.\")  # unsafe deserialization\n\nimport requests\nrequests.get(\"https://example.com\", verify=False)  # disables SSL verification\n\n", "completion": "Unsafe eval usage | Unsafe exec usage | Unsafe pickle deserialization | Insecure requests verify=False | Hardcoded credentials"}
{"prompt": "Analyze this Python code for vulnerabilities:\ndef add(a, b):\n    return a + b\n\nnumbers = [1,2,3,4,5]\ntotal = sum(numbers)\nprint(f\"Total: {total}\")\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Python code for vulnerabilities:\nimport sqlite3\n\nconn = sqlite3.connect(\"users.db\")\nc = conn.cursor()\n\nusername = input(\"Enter username: \")\npassword = input(\"Enter password: \")\n\n# Vulnerable query\nquery = \"SELECT * FROM users WHERE username = '{}' AND password = '{}'\".format(username, password)\nprint(\"Executing query:\", query)\nc.execute(query)\nrows = c.fetchall()\n\nfor row in rows:\n    print(row)\n\nconn.close()\n\n", "completion": "Hardcoded credentials"}
{"prompt": "Analyze this Python code for vulnerabilities:\nimport pymysql\n\ndb = pymysql.connect(\"localhost\",\"user\",\"pass\",\"testdb\")\ncursor = db.cursor()\n\nsearch = input(\"Search product: \")\n\n# Vulnerable concatenation\nsql = \"SELECT * FROM products WHERE name LIKE '%\" + search + \"%';\"\ncursor.execute(sql)\n\nfor row in cursor.fetchall():\n    print(row)\n\ndb.close()\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Python code for vulnerabilities:\n#!/usr/bin/env python3\n\"\"\"\nchat.py \u2014 dataset + heuristic vulnerability analyzer (no LLM).\n\nUsage:\n  chat.py -f some_file.py\n  chat.py -f some_folder/\n  chat.py -p \"some code snippet or keywords\"\n\nIt will:\n - Try to fuzzy-match file contents against vuln_dataset.jsonl entries (best-effort).\n - If no good dataset match, run a set of heuristic checks per language and return findings.\n - Log results to chat_log.txt.\n\"\"\"\nimport argparse\nimport os\nimport sys\nimport json\nimport re\nimport difflib\nfrom datetime import datetime\n\n# Config\nDATASET_FILE = \"vuln_dataset.jsonl\"\nLOG_FILE = \"chat_log.txt\"\nMAX_FILE_SIZE = 5 * 1024 * 1024  # 5 MB\nFUZZY_MATCH_THRESHOLD = 0.60\n\n# Supported extensions -> language labels\nEXT_LANG_MAP = {\n    \".py\": \"Python\",\n    \".c\": \"C\",\n    \".cpp\": \"C++\",\n    \".h\": \"C/C++ Header\",\n    \".java\": \"Java\",\n    \".js\": \"JavaScript\",\n    \".go\": \"Go\",\n    \".rs\": \"Rust\",\n    \".php\": \"PHP\",\n    \".sh\": \"Shell\",\n}\n\n# Load dataset (if available)\nvuln_dataset = []\nif os.path.exists(DATASET_FILE):\n    try:\n        with open(DATASET_FILE, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    vuln_dataset.append(json.loads(line))\n                except json.JSONDecodeError:\n                    continue\n    except Exception as e:\n        print(f\"[Warning] Could not read dataset file '{DATASET_FILE}': {e}\")\n\ndef save_to_log(report_text, prompt_desc):\n    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n        f.write(f\"Timestamp: {ts}\\n\")\n        f.write(f\"Input: {prompt_desc}\\n\")\n        f.write(\"-\"*80 + \"\\n\")\n        f.write(report_text + \"\\n\")\n        f.write(\"=\"*80 + \"\\n\\n\")\n\ndef fuzzy_match_score(a, b):\n    return difflib.SequenceMatcher(None, a, b).ratio()\n\ndef find_dataset_match(file_content):\n    \"\"\"Return best dataset completion and score, or (None, 0).\"\"\"\n    best_score = 0.0\n    best_completion = None\n    for entry in vuln_dataset:\n        prompt = entry.get(\"prompt\", \"\")\n        code_part = prompt.split(\"\\n\", 1)[1].strip() if \"\\n\" in prompt else prompt\n        score = fuzzy_match_score(file_content, code_part)\n        if score > best_score:\n            best_score = score\n            best_completion = entry.get(\"completion\", \"\")\n    if best_score >= FUZZY_MATCH_THRESHOLD:\n        return best_completion, best_score\n    return None, 0.0\n\n#########################\n# Heuristic checks\n#########################\n\ndef check_python(content):\n    findings = []\n    if re.search(r\"\\beval\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Unsafe eval\",\n            \"description\": \"Use of eval() can execute arbitrary code.\",\n            \"fix\": \"Avoid eval(); validate inputs or use safer parsing.\"\n        })\n    if re.search(r\"\\bexec\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Unsafe exec\",\n            \"description\": \"Use of exec() can execute arbitrary code.\",\n            \"fix\": \"Avoid exec(); use safer alternatives.\"\n        })\n    if re.search(r\"subprocess\\..+\\bshell\\s*=\\s*True\", content):\n        findings.append({\n            \"type\": \"Subprocess shell=True\",\n            \"description\": \"Using subprocess with shell=True can allow shell injection.\",\n            \"fix\": \"Use list arguments and avoid shell=True.\"\n        })\n    if re.search(r\"\\bpickle\\.(loads|load)\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Unsafe deserialization (pickle)\",\n            \"description\": \"Untrusted pickle data can execute arbitrary code.\",\n            \"fix\": \"Do not unpickle untrusted data.\"\n        })\n    if re.search(r\"requests\\.(get|post)\\s*\\(.*verify\\s*=\\s*False\", content):\n        findings.append({\n            \"type\": \"Insecure TLS verification\",\n            \"description\": \"requests called with verify=False disables SSL verification.\",\n            \"fix\": \"Do not set verify=False; use proper certificates.\"\n        })\n    return findings\n\ndef check_c_cpp(content):\n    findings = []\n    if re.search(r\"\\bstrcpy\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Buffer overflow (strcpy)\",\n            \"description\": \"Use of strcpy can overflow buffers.\",\n            \"fix\": \"Use strncpy or bounds-checked functions.\"\n        })\n    if re.search(r\"\\bgets\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Buffer overflow (gets)\",\n            \"description\": \"gets() is unsafe.\",\n            \"fix\": \"Use fgets() with bounds checking.\"\n        })\n    if re.search(r\"\\bsystem\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Untrusted command execution (system)\",\n            \"description\": \"system() with untrusted data can lead to command injection.\",\n            \"fix\": \"Avoid system(); sanitize inputs.\"\n        })\n    return findings\n\ndef check_java(content):\n    findings = []\n    lines = content.splitlines()\n    for i, line in enumerate(lines, 1):  # start line numbers at 1\n        if re.search(r\"\\bRuntime\\.getRuntime\\(\\)\\.exec\\s*\\(\", line):\n            findings.append({\n                \"type\": \"Command execution (Runtime.exec)\",\n                \"description\": \"Runtime.exec can run system commands; be careful with untrusted input.\",\n                \"fix\": \"Validate and sanitize inputs; prefer safer APIs.\",\n                \"line\": i\n            })\n        if re.search(r\"\\bObjectInputStream\\b\", line):\n            findings.append({\n                \"type\": \"Potential deserialization risk\",\n                \"description\": \"Java serialization can be exploited if processing untrusted data.\",\n                \"fix\": \"Avoid Java serialization for untrusted data.\",\n                \"line\": i\n            })\n        if re.search(r'String\\s+\\w+\\s*=\\s*\".*\"\\s*\\+\\s*\\w+', line):\n            findings.append({\n                \"type\": \"SQL Injection risk\",\n                \"description\": \"User input concatenated into SQL string can lead to SQL injection.\",\n                \"fix\": \"Use PreparedStatement instead of concatenation.\",\n                \"line\": i\n            })\n        if re.search(r'String\\s+\\w*password\\w*\\s*;', line, re.IGNORECASE):\n            findings.append({\n                \"type\": \"Plaintext password storage\",\n                \"description\": \"Storing passwords in plaintext is unsafe.\",\n                \"fix\": \"Hash passwords before storing (bcrypt or similar).\",\n                \"line\": i\n            })\n        if re.search(r'new\\s+File\\s*\\(\\s*\\w+\\s*\\)', line):\n            findings.append({\n                \"type\": \"Unsafe file handling\",\n                \"description\": \"User-controlled filename used without validation.\",\n                \"fix\": \"Validate file paths and restrict access.\",\n                \"line\": i\n            })\n    return findings\n\ndef check_javascript(content):\n    findings = []\n    if re.search(r\"\\beval\\s*\\(\", content):\n        findings.append({\n            \"type\": \"Unsafe eval\",\n            \"description\": \"Use of eval() can execute arbitrary JS.\",\n            \"fix\": \"Avoid eval(); sanitize inputs.\"\n        })\n    if re.search(r\"exec\\(\", content) or re.search(r\"child_process\\..*exec\", content):\n        findings.append({\n            \"type\": \"Command execution (exec)\",\n            \"description\": \"Child process exec with untrusted input can lead to command injection.\",\n            \"fix\": \"Use spawn with argument arrays; sanitize inputs.\"\n        })\n    if re.search(r\"document\\.write\\s*\\(\", content):\n        findings.append({\n            \"type\": \"DOM injection (document.write)\",\n            \"description\": \"document.write with untrusted input can lead to XSS.\",\n            \"fix\": \"Escape or sanitize content before DOM insertion.\"\n        })\n    return findings\n\ndef check_shell(content):\n    findings = []\n    if re.search(r\"eval\\s+\", content) or re.search(r\"`.*`\", content):\n        findings.append({\n            \"type\": \"Shell eval/backticks\",\n            \"description\": \"Using eval/backticks in shell can be dangerous.\",\n            \"fix\": \"Avoid eval; sanitize inputs.\"\n        })\n    return findings\n\ndef check_go(content):\n    findings = []\n    if re.search(r\"\\bexec\\.Command\", content):\n        findings.append({\n            \"type\": \"Command execution\",\n            \"description\": \"exec.Command can run system commands; watch for untrusted inputs.\",\n            \"fix\": \"Sanitize inputs and prefer safer APIs.\"\n        })\n    return findings\n\ndef language_checks(language, content):\n    if language == \"Python\":\n        return check_python(content)\n    if language in (\"C\", \"C++\", \"C/C++ Header\"):\n        return check_c_cpp(content)\n    if language == \"Java\":\n        return check_java(content)\n    if language == \"JavaScript\":\n        return check_javascript(content)\n    if language == \"Shell\":\n        return check_shell(content)\n    if language == \"Go\":\n        return check_go(content)\n    return []\n\n#########################\n# File analysis\n#########################\n\ndef detect_language_from_ext(path):\n    ext = os.path.splitext(path)[1].lower()\n    return EXT_LANG_MAP.get(ext, None)\n\ndef gather_files(paths):\n    \"\"\"Accept files or directories and return list of files to analyze.\"\"\"\n    all_files = []\n    for p in paths:\n        if os.path.isfile(p):\n            all_files.append(p)\n        elif os.path.isdir(p):\n            for root, _, files in os.walk(p):\n                for f in files:\n                    all_files.append(os.path.join(root, f))\n        else:\n            print(f\"[Error] Path not found: {p}\")\n    return all_files\n\ndef analyze_file(path):\n    if not os.path.exists(path):\n        return {\"error\": f\"File not found: {path}\"}\n    size = os.path.getsize(path)\n    if size > MAX_FILE_SIZE:\n        return {\"error\": f\"File too large ({size} bytes) \u2014 max is {MAX_FILE_SIZE} bytes.\"}\n    try:\n        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            content = f.read()\n    except Exception as e:\n        return {\"error\": f\"Could not read file: {e}\"}\n\n    lang = detect_language_from_ext(path) or \"Unknown\"\n\n    # Run dataset match\n    ds_completion, ds_score = find_dataset_match(content)\n\n    # Run heuristic checks\n    findings = language_checks(lang, content)\n\n    # Add generic hardcoded password check\n    if re.search(r\"(password\\s*[:=]\\s*['\\\"]\\w+['\\\"]|passwd\\s*[:=]\\s*['\\\"]\\w+['\\\"])\", content, re.IGNORECASE):\n        findings.append({\n            \"type\": \"Hardcoded credentials\",\n            \"description\": \"Hardcoded passwords or credentials detected.\",\n            \"fix\": \"Store secrets in environment variables or a secrets manager.\"\n        })\n\n    # If no findings\n    if not findings:\n        findings = []\n\n    # Combine results\n    result = {\n        \"language\": lang,\n        \"method\": \"heuristics\" if findings else \"dataset_match\",\n        \"dataset_match\": ds_completion if ds_completion else None,\n        \"dataset_score\": ds_score if ds_completion else None,\n        \"findings\": findings if findings else [{\"note\": \"No obvious issues found.\"}]\n    }\n\n    return result\n\ndef analyze_text_snippet(snippet):\n    ds_completion, ds_score = find_dataset_match(snippet)\n    if ds_completion:\n        return {\"method\": \"dataset_match\", \"dataset_score\": ds_score, \"finding\": ds_completion}\n    return {\"method\": \"heuristics\", \"note\": \"No dataset match found.\"}\n\n#########################\n# Main\n#########################\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Chat using local dataset/heuristics for vulnerability scanning\")\n    parser.add_argument(\"-f\", \"--file\", nargs=\"+\", help=\"Code file(s) or folder(s) to analyze\")\n    parser.add_argument(\"-p\", \"--prompt\", help=\"Short code snippet or keywords to check against dataset\")\n    parser.add_argument(\"--view-log\", action=\"store_true\", help=\"View chat log\")\n    parser.add_argument(\"--clear-log\", action=\"store_true\", help=\"Clear chat log\")\n    args = parser.parse_args()\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        return\n\n    if args.view_log:\n        if os.path.exists(LOG_FILE):\n            print(open(LOG_FILE, \"r\", encoding=\"utf-8\").read())\n        else:\n            print(\"No log file.\")\n        return\n\n    if args.clear_log:\n        open(LOG_FILE, \"w\").close()\n        print(\"Log cleared.\")\n        return\n\n    if args.file:\n        files_to_analyze = gather_files(args.file)\n        if not files_to_analyze:\n            print(\"[Error] No valid files found.\")\n            parser.print_help()\n            return\n        for path in files_to_analyze:\n            result = analyze_file(path)\n            print(f\"\\n--- Analysis for {path} ---\\n{json.dumps(result, indent=2)}\\n\")\n            save_to_log(json.dumps(result, indent=2), f\"File: {path}\")\n\n    elif args.prompt:\n        result = analyze_text_snippet(args.prompt)\n        pretty = json.dumps(result, indent=2)\n        print(pretty)\n        save_to_log(pretty, f\"Prompt: {args.prompt}\")\n\n    else:\n        print(\"[Error] Unknown or missing command.\")\n        parser.print_help()\n\nif __name__ == \"__main__\":\n    main()\n\n", "completion": "Unsafe eval usage | Unsafe exec usage"}
{"prompt": "Analyze this Python code for vulnerabilities:\n#!/usr/bin/env python3\nimport argparse\nimport subprocess\nimport sys\nimport os\nimport json\nfrom datetime import datetime\nimport yaml\nfrom dotenv import dotenv_values\nimport time\n\n# Constants\nCLI_BIN_LOC = \"/home/modeluser/BitNet/build/bin/llama-cli\"\nMODEL_LOC = \"/home/modeluser/BitNet/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf\"\nSYSTEM_PROMPT = \"You are a helpful AI assistant integrated into a DevSecOps platform. Provide concise, technical responses.\"\nLOG_FILE = \"chat_log.txt\"\nTEMP_OUTPUT = \"chat_temp.txt\"\nSESSION_TRACKER_FILE = '/tmp/chat_session.cfg'\nMAX_FILE_SIZE = 5000  # Approx. 1200 tokens, safe for a 2k context window\nMAPPING_SAMPLE_BUDGET = 5000 # Max total characters for all samples in Pass 1\nCHUNK_TARGET_SIZE = 3500 # Target size for each chunk 1k token\nPASS_2_ANALYSIS_OUTPUT = \"/tmp/chat_pass2_analysis.txt\" # store the combined findings from this pass.\n\nPASS_1_MAP_OUTPUT = \"/tmp/chat_pass1_map.json\" # Output file for Pass 1 mapping\n\n# LLM execution defaults\nretries = 3\ntimeout = 120\nshow_thinking_indicator = True\n\n\ndef generate_chunk_manifest(file_paths, chunk_target_size=CHUNK_TARGET_SIZE):\n    \"\"\"\n    Implements the \"Card Catalog\" generator.\n    Scans files to create a manifest of chunk locations (byte offsets)\n    without loading the file contents into memory.\n    \"\"\"\n    manifest = []\n    chunk_id_counter = 0\n    for file_path in file_paths:\n        if not os.path.exists(file_path):\n            continue\n        try:\n            # Open with 'rb' to accurately tell byte positions\n            with open(file_path, 'rb') as f:\n                # Read the whole content once for splitting, but decode for processing.\n                # This is a pragmatic choice; it avoids complex byte-stream parsing\n                # and solves the primary problem: storing all chunks in memory.\n                content = f.read().decode('utf-8', errors='ignore')\n                paragraphs = content.split('\\n\\n')\n                current_chunk_str = \"\"\n                chunk_start_byte = 0\n\n                for paragraph in paragraphs:\n                    if not paragraph.strip():\n                        # Even empty paragraphs consume bytes (newlines)\n                        chunk_start_byte += len(paragraph.encode('utf-8', errors='ignore')) + 2 # for '\\n\\n'\n                        continue\n\n                    # If adding the next paragraph makes the chunk too big, finalize the current one\n                    if len(current_chunk_str) + len(paragraph) > chunk_target_size and current_chunk_str:\n                        chunk_byte_len = len(current_chunk_str.encode('utf-8', errors='ignore'))\n                        manifest.append({\n                            \"id\": chunk_id_counter,\n                            \"file_path\": file_path,\n                            \"start_byte\": chunk_start_byte,\n                            \"end_byte\": chunk_start_byte + chunk_byte_len\n                        })\n                        chunk_id_counter += 1\n                        chunk_start_byte += chunk_byte_len\n                        current_chunk_str = \"\"\n\n                    current_chunk_str += paragraph + \"\\n\\n\"\n\n                # Add the last remaining chunk\n                if current_chunk_str:\n                    chunk_byte_len = len(current_chunk_str.encode('utf-8', errors='ignore'))\n                    manifest.append({\n                        \"id\": chunk_id_counter,\n                        \"file_path\": file_path,\n                        \"start_byte\": chunk_start_byte,\n                        \"end_byte\": chunk_start_byte + chunk_byte_len\n                    })\n                    chunk_id_counter += 1\n            return manifest\n        except Exception as e:\n            print(f\"Warning: Could not create chunk manifest for file {file_path}: {e}\", file=sys.stderr)\n            return []\n\ndef read_chunks_from_manifest(manifest, selected_ids):\n    \"\"\"\n    Performs targeted reading of chunks from the original files using the manifest.\n    This is the \"just-in-time\" content loading part of the \"Card Catalog\" architecture.\n    \"\"\"\n    chunk_contents = {}\n    # Create a quick lookup map for the selected IDs\n    id_to_chunk_meta = {chunk['id']: chunk for chunk in manifest if chunk['id'] in selected_ids}\n\n    for chunk_id in selected_ids:\n        if chunk_id in id_to_chunk_meta:\n            meta = id_to_chunk_meta[chunk_id]\n            try:\n                with open(meta['file_path'], 'rb') as f:\n                    f.seek(meta['start_byte'])\n                    content_bytes = f.read(meta['end_byte'] - meta['start_byte'])\n                    chunk_contents[chunk_id] = content_bytes.decode('utf-8', errors='ignore')\n            except Exception as e:\n                print(f\"Warning: Could not read chunk id {chunk_id} from {meta['file_path']}: {e}\", file=sys.stderr)\n    return chunk_contents\n\n\n\nimport itertools\nimport threading\nimport time\n\ndef run_non_interactive_analysis(args, settings):\n    \"\"\"\n    Orchestrates the multi-pass analysis for non-interactive mode.\n    Handles both small files (single-pass) and large files (multi-pass).\n    \"\"\"\n\n    # Clean up any leftover files from a previous run\n    cleanup_temp_files()\n\n    # --- Step 1: Calculate total size and decide on a strategy ---\n    total_size = 0\n    final_answer = \"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    if args.file:\n        for path in args.file:\n            if os.path.exists(path):\n                total_size += os.path.getsize(path)\n\n    # --- STRATEGY 1: Small files, single-pass ---\n    if total_size <= MAX_FILE_SIZE:\n        print(\"File size is within the single-pass limit. Performing direct analysis...\")\n        full_file_content = \"\"\n        if args.file:\n            for path in args.file:\n                if os.path.exists(path):\n                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                        full_file_content += f\"\\n\\n--- START OF FILE: {os.path.basename(path)} ---\\n\"\n                        full_file_content += f.read()\n                        full_file_content += f\"\\n--- END OF FILE: {os.path.basename(path)} ---\\n\"\n        user_prompt = args.prompt + full_file_content\n        formatted_prompt = format_prompt(settings['system_prompt'], user_prompt)\n        print(\"[Pass 1/1] Generating final analysis...\")\n        \n        output = call_llm(\n            prompt=formatted_prompt,\n            cli_bin_loc=settings['cli_bin_loc'],\n            model_loc=settings['model_loc'],\n            max_tokens=settings.get('tokens'),\n            show_thinking=show_thinking_indicator,\n            temperature=settings.get('temperature'),\n            top_p=settings.get('top_p'),\n            repeat_penalty=settings.get('repeat_penalty'),\n            ctx_size=settings.get('ctx_size'),\n            seed=settings.get('seed'),\n            threads=settings.get('threads')\n        )\n\n        save_to_log(output, args.prompt, timestamp)\n        final_answer = extract_response()\n        print(\"\\n--- Final Answer ---\")\n        if args.full_verbose: print(output)\n        else: print(final_answer)\n\n    # --- STRATEGY 2: Large files, multi-pass ---\n    else:\n        print(f\"Total file size ({total_size / 1024:.1f} KB) exceeds limit. Starting multi-pass analysis.\")\n\n        # === PASS 1: MAPPING ===\n        print(\"\\n[Pass 1/3] Mapping document structure (one by one)...\")\n        final_map = {}\n\n        def spinner_task(stop_event):\n            for c in itertools.cycle('|/-\\\\'):\n                if stop_event.is_set(): break\n                print(f'\\r{spinner_msg} {c}', end='', flush=True)\n                time.sleep(0.1)\n            print('\\r' + ' ' * (len(spinner_msg) + 2) + '\\r', end='', flush=True)\n\n        if args.file:\n            for path in args.file:\n                if not os.path.exists(path): continue\n                spinner_msg = f\"  - Mapping {os.path.basename(path)}...\"\n                stop_spinner = threading.Event()\n                t = threading.Thread(target=spinner_task, args=(stop_spinner,))\n                t.start()\n\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                    sample_content = f.read(MAPPING_SAMPLE_BUDGET)\n                map_prompt_content = (\"Analyze the beginning of the following document and create a structural map. \"\n                                      \"Provide a one-sentence summary and a list of key topics or sections. \"\n                                      \"Respond ONLY with a single, minified JSON object containing 'summary' and 'topics' keys.\\n\\n\"\n                                      f\"--- DOCUMENT SAMPLE ---\\n{sample_content}\")\n                mapping_prompt = format_prompt(settings['system_prompt'], map_prompt_content)\n                map_output_raw = call_llm(\n                    prompt=mapping_prompt, cli_bin_loc=settings['cli_bin_loc'], model_loc=settings['model_loc'],\n                    max_tokens=1024, show_thinking=False, temperature=0.2\n                )\n\n                stop_spinner.set()\n                t.join()\n                print(f\"  - Mapping {os.path.basename(path)}... Done\")\n\n                log_prompt_desc = f\"[PASS 1 - MAPPING on {os.path.basename(path)}] User Query: {args.prompt}\"\n                save_to_log(map_output_raw, log_prompt_desc, timestamp)\n                \n                map_json_str = extract_response()\n                try:\n                    final_map[os.path.basename(path)] = json.loads(map_json_str)\n                except json.JSONDecodeError:\n                    print(f\"    Warning: Could not parse map for {os.path.basename(path)}. Skipping.\")\n\n        try:\n            with open(PASS_1_MAP_OUTPUT, 'w', encoding='utf-8') as f:\n                json.dump(final_map, f, indent=2)\n            print(f\"Successfully created and saved complete document map to {PASS_1_MAP_OUTPUT}\")\n            parsed_map = final_map\n        except Exception as e:\n            print(f\"\\n[Error] Could not save the final document map: {e}\"); sys.exit(1)\n\n        # === PASS 2: ANALYSIS ===\n        # ... (rest of your multi-pass analysis code remains unchanged) ...\n\n        # === PASS 2: ANALYSIS (REBUILT WITH \"CARD CATALOG\" ARCHITECTURE) ===\n        print(\"\\n[Pass 2/3] Analyzing relevant document sections...\")\n\n        # Step 2a: Generate the lightweight chunk manifest without loading content.\n        print(\"  - Generating chunk manifest (Card Catalog)...\")\n        chunk_manifest = generate_chunk_manifest(args.file)\n        if not chunk_manifest: print(\"Could not generate a chunk manifest. Aborting.\"); sys.exit(1)\n        print(f\"  - Found {len(chunk_manifest)} total chunks across all files.\")\n\n        # Step 2b: Ask the LLM to select relevant chunk IDs based on the manifest.\n        map_str = json.dumps(parsed_map, indent=2)\n        lightweight_chunk_info = json.dumps([{\"id\": chunk[\"id\"], \"file\": os.path.basename(chunk[\"file_path\"])} for chunk in chunk_manifest])\n        \n        selection_prompt = (f\"User Query: '{args.prompt}'\\n\\n\"\n                            f\"Document Map:\\n{map_str}\\n\\n\"\n                            f\"Available Chunks:\\n{lightweight_chunk_info}\\n\\n\"\n                            \"Based on the user query and the map, which chunk 'id's are most likely to contain the answer? \"\n                            \"Be selective to stay focused. Respond ONLY with a single, minified JSON list of integer ids.\")\n        selection_formatted_prompt = format_prompt(settings['system_prompt'], selection_prompt)\n        selected_chunks_raw = call_llm(\n            prompt=selection_formatted_prompt, cli_bin_loc=settings['cli_bin_loc'], model_loc=settings['model_loc'],\n            max_tokens=512, show_thinking=show_thinking_indicator, temperature=0.1\n        )\n        # ADDED LOGGING\n        save_to_log(selected_chunks_raw, f\"[PASS 2 - SELECTION] User Query: {args.prompt}\", timestamp)\n        \n        try:\n            selected_indices = json.loads(extract_response())\n            if not isinstance(selected_indices, list): raise ValueError(\"Not a list\")\n            print(f\"LLM selected {len(selected_indices)} relevant chunks: {selected_indices}\")\n        except (json.JSONDecodeError, ValueError) as e:\n            print(f\"\\n[Error] Could not parse the list of selected chunks from the LLM: {e}\"); print(\"Raw output:\", extract_response()); sys.exit(1)\n        \n        if not selected_indices: print(\"The LLM determined that no specific chunks are relevant to the query. Cannot proceed.\"); sys.exit(0)\n\n        # Step 2c: Read ONLY the selected chunks' content from disk.\n        print(\"  - Performing targeted read of selected chunk contents...\")\n        selected_chunk_contents = read_chunks_from_manifest(chunk_manifest, selected_indices)\n\n        # Step 2d: Analyze the selected content in batches.\n        ANALYSIS_BATCH_CHAR_BUDGET = 4000\n        current_batch_content = \"\"\n        open(PASS_2_ANALYSIS_OUTPUT, 'w').close()\n        \n        for chunk_id in selected_indices:\n            if chunk_id not in selected_chunk_contents:\n                print(f\"Warning: LLM selected chunk id {chunk_id}, which could not be read. Skipping.\")\n                continue\n\n            chunk_content = selected_chunk_contents[chunk_id]\n            if len(current_batch_content) + len(chunk_content) > ANALYSIS_BATCH_CHAR_BUDGET:\n                if current_batch_content:\n                    analysis_prompt = (f\"User Query: '{args.prompt}'\\n\\n\"\n                                       \"Extract all information, facts, and findings relevant to the user query from the following text. Be detailed and quote important parts.\\n\\n--- TEXT ---\\n\"\n                                       f\"{current_batch_content}\")\n                    analysis_formatted_prompt = format_prompt(settings['system_prompt'], analysis_prompt)\n                    findings_raw = call_llm(prompt=analysis_formatted_prompt, cli_bin_loc=settings['cli_bin_loc'], model_loc=settings['model_loc'], max_tokens=1024)\n                    # ADDED LOGGING\n                    save_to_log(findings_raw, f\"[PASS 2 - ANALYSIS BATCH] User Query: {args.prompt}\", timestamp)\n                    findings = extract_response()\n                    with open(PASS_2_ANALYSIS_OUTPUT, 'a', encoding='utf-8') as f: f.write(findings + \"\\n\\n\")\n                current_batch_content = chunk_content\n            else:\n                current_batch_content += chunk_content + \"\\n\\n\"\n\n        if current_batch_content: # Process the final batch\n            analysis_prompt = (f\"User Query: '{args.prompt}'\\n\\n\"\n                               \"Extract all information, facts, and findings relevant to the user query from the following text. Be detailed and quote important parts.\\n\\n--- TEXT ---\\n\"\n                               f\"{current_batch_content}\")\n            analysis_formatted_prompt = format_prompt(settings['system_prompt'], analysis_prompt)\n            findings_raw = call_llm(prompt=analysis_formatted_prompt, cli_bin_loc=settings['cli_bin_loc'], model_loc=settings['model_loc'], max_tokens=1024)\n            # ADDED LOGGING\n            save_to_log(findings_raw, f\"[PASS 2 - ANALYSIS FINAL BATCH] User Query: {args.prompt}\", timestamp)\n            findings = extract_response()\n            with open(PASS_2_ANALYSIS_OUTPUT, 'a', encoding='utf-8') as f: f.write(findings + \"\\n\\n\")\n\n        print(f\"Analysis of relevant chunks complete. Findings saved to {PASS_2_ANALYSIS_OUTPUT}\")\n        \n        # === PASS 3: SYNTHESIS (Remains the same) ===\n        print(\"\\n[Pass 3/3] Synthesizing final answer...\")\n        try:\n            with open(PASS_2_ANALYSIS_OUTPUT, 'r', encoding='utf-8') as f: findings_text = f.read()\n        except FileNotFoundError: print(\"\\n[Error] Findings file not found. Cannot synthesize.\"); sys.exit(1)\n        if not findings_text.strip(): print(\"\\nNo findings were extracted in the analysis phase. Cannot provide a final answer.\"); sys.exit(0)\n        if len(findings_text) > ANALYSIS_BATCH_CHAR_BUDGET:\n            print(f\"Warning: Collected findings are very large. Truncating for final synthesis.\")\n            findings_text = findings_text[:ANALYSIS_BATCH_CHAR_BUDGET]\n        synthesis_prompt_content = (f\"Your task is to answer the user's original query based on the provided research notes. Do not just list the notes; synthesize them into a single, coherent, well-structured final answer.\\n\\n\"\n                                    f\"USER'S ORIGINAL QUERY: '{args.prompt}'\\n\\n\"\n                                    f\"--- RESEARCH NOTES AND FINDINGS ---\\n\"\n                                    f\"{findings_text}\")\n        synthesis_prompt = format_prompt(settings['system_prompt'], synthesis_prompt_content)\n        final_output_raw = call_llm(\n            prompt=synthesis_prompt, cli_bin_loc=settings['cli_bin_loc'], model_loc=settings['model_loc'],\n            max_tokens=settings.get('tokens'), show_thinking=show_thinking_indicator, temperature=0.7,\n            top_p=settings.get('top_p'), repeat_penalty=settings.get('repeat_penalty'),\n            ctx_size=settings.get('ctx_size'), seed=settings.get('seed'), threads=settings.get('threads')\n        )\n        # ADDED LOGGING\n        save_to_log(final_output_raw, f\"[PASS 3 - SYNTHESIS] User Query: {args.prompt}\", timestamp)\n        final_answer = extract_response()\n        print(\"\\n--- Final Answer ---\")\n        if args.full_verbose: print(final_output_raw)\n        else: print(final_answer)\n\n    if args.show_work:\n        print(\"\\n\\n--- Intermediate Work ---\")\n        # Check if the multi-pass strategy was used (i.e., if files were large)\n        if total_size > MAX_FILE_SIZE:\n            map_exists = os.path.exists(PASS_1_MAP_OUTPUT)\n            findings_exist = os.path.exists(PASS_2_ANALYSIS_OUTPUT)\n\n            if not map_exists and not findings_exist:\n                print(\"Multi-pass strategy was used, but no intermediate files were found (an error may have occurred).\")\n            \n            if map_exists:\n                print(f\"\\n--- Document Map (from {PASS_1_MAP_OUTPUT}) ---\")\n                with open(PASS_1_MAP_OUTPUT, 'r', encoding='utf-8') as f:\n                    print(f.read())\n            \n            if findings_exist:\n                print(f\"\\n--- Raw Findings (from {PASS_2_ANALYSIS_OUTPUT}) ---\")\n                with open(PASS_2_ANALYSIS_OUTPUT, 'r', encoding='utf-8') as f:\n                    print(f.read())\n        else:\n            # This is the new, helpful message for single-pass runs\n            print(\"(--show-work is active, but no intermediate files were generated because the input was handled by the more efficient single-pass strategy.)\")\n\ndef cleanup_temp_files():\n    \"\"\"Deletes temporary pass files to ensure a clean run.\"\"\"\n    files_to_delete = [PASS_1_MAP_OUTPUT, PASS_2_ANALYSIS_OUTPUT]\n    for f in files_to_delete:\n        if os.path.exists(f):\n            try:\n                os.remove(f)\n            except OSError as e:\n                print(f\"Warning: Could not delete temp file {f}: {e}\", file=sys.stderr)\n\ndef save_to_log(output, prompt, timestamp):\n    \"\"\"Save output to log file with timestamp and prompt\"\"\"\n    with open(LOG_FILE, 'a') as f:\n        f.write(f\"\\n{'='*60}\\n\")\n        f.write(f\"Timestamp: {timestamp}\\n\")\n        f.write(f\"Prompt: {prompt}\\n\")\n        f.write(f\"{'='*60}\\n\")\n        f.write(output)\n        f.write(f\"\\n{'='*60}\\n\\n\")\n\ndef load_configuration(file_path):\n    config = {}\n    try:\n        if file_path.endswith(('.yaml', '.yml')):\n            with open(file_path, 'r', encoding='utf-8') as f:\n                config = yaml.safe_load(f)\n        elif file_path.endswith('.env'):\n            config = dotenv_values(file_path)\n        if config:\n            return {str(k).lower(): v for k, v in config.items()}\n        return {}\n    except FileNotFoundError:\n        print(f\"Warning: Configuration file not found at '{file_path}'\", file=sys.stderr)\n        return {}\n    except Exception as e:\n        print(f\"Warning: Error parsing configuration file '{file_path}': {e}\", file=sys.stderr)\n        return {}\n\ndef get_session_config_path():\n    try:\n        if not os.path.exists(SESSION_TRACKER_FILE):\n            return None\n        with open(SESSION_TRACKER_FILE, 'r', encoding='utf-8') as f:\n            path = f.read().strip()\n            return path if path else None\n    except Exception as e:\n        print(f\"Warning: Could not read session tracker file: {e}\", file=sys.stderr)\n        return None\n\ndef save_session_config_path(path):\n    if not os.path.isfile(path):\n        print(f\"Error: Configuration file not found at '{path}'\", file=sys.stderr)\n        return False\n    try:\n        with open(SESSION_TRACKER_FILE, 'w', encoding='utf-8') as f:\n            f.write(path)\n        return True\n    except Exception as e:\n        print(f\"Error: Could not write to session tracker file: {e}\", file=sys.stderr)\n        return False\n\ndef clear_session_config_path():\n    try:\n        with open(SESSION_TRACKER_FILE, 'w'):\n            pass\n        print(\"Session configuration has been reset to defaults.\")\n    except Exception as e:\n        print(f\"Error: Could not clear session tracker file: {e}\", file=sys.stderr)\n\n#######################################\n# File Utilities\n#######################################\n\ndef validate_environment_and_inputs(file_paths=None):\n    if not os.path.isfile(CLI_BIN_LOC):\n        print(f\"[Error] LLM CLI not found at {CLI_BIN_LOC}\")\n        sys.exit(1)\n    if not os.path.isfile(MODEL_LOC):\n        print(f\"[Error] Model file not found at {MODEL_LOC}\")\n        sys.exit(1)\n    if file_paths:\n        for path in file_paths:\n            if not os.path.isfile(path):\n                print(f\"[Warning] File '{path}' not found. Skipping.\")\n\ndef read_file_in_chunks(file_path, chunk_size=MAX_FILE_SIZE):\n    if not os.path.exists(file_path):\n        print(f\"[Warning] File '{file_path}' not found.\")\n        return\n    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n        while True:\n            data = f.read(chunk_size)\n            if not data:\n                break\n            yield data\n\ndef view_file(file_path):\n    if not os.path.exists(file_path):\n        print(f\"[Warning] File '{file_path}' not found.\")\n        return\n    size_kb = os.path.getsize(file_path) / 1024\n    chunks = list(read_file_in_chunks(file_path))\n    if not chunks:\n        return\n    print(f\"File Size: {size_kb:.0f} KB\")\n    print(f\"--- {file_path} ---\")\n    for i, chunk in enumerate(chunks, start=1):\n        print(chunk.strip())\n        if i == len(chunks):\n            print(f\"--- End ---\\n\")\n\ndef clear_file(file_path):\n    try:\n        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n            print(f\"The file '{file_path}' is already empty or does not exist.\")\n            return\n        confirm = input(f\"Are you sure you want to clear '{file_path}'? [y/N]: \")\n        if confirm.lower() == 'y':\n            open(file_path, 'w').close()\n            print(f\"Successfully cleared '{file_path}'.\")\n        else:\n            print(\"Operation cancelled.\")\n    except Exception as e:\n        print(f\"Error clearing file: {e}\")\n\ndef view_file_contents(file_paths):\n    image_exts = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'}\n    for path in file_paths:\n        ext = os.path.splitext(path)[1].lower()\n        if ext in image_exts:\n            print(f\"Skipping '{path}' (I don't open image files).\")\n            continue\n        view_file(path)\n\n#######################################\n# LLM Utilities\n#######################################\n\ndef format_prompt(system_prompt, user_prompt):\n    return f\"\"\"<|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\n{user_prompt}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\ndef call_llm(prompt, cli_bin_loc, model_loc, max_tokens=512, show_thinking=True, temperature=0.8, top_p=0.9, repeat_penalty=1.0, ctx_size=2048, seed=None, threads=None):\n    stop_spinner = None\n    if show_thinking:\n        stop_spinner = spinner(\"Thinking...\")\n\n    cmd = [\n        cli_bin_loc,\n        \"-m\", model_loc,\n        \"-p\", prompt,\n        \"--n-predict\", str(max_tokens),\n        \"--temp\", str(temperature),\n        \"--top-p\", str(top_p),\n        \"--repeat-penalty\", str(repeat_penalty),\n        \"-c\", str(ctx_size)\n    ]\n    if seed is not None: cmd.extend([\"--seed\", str(seed)])\n    if threads is not None: cmd.extend([\"--threads\", str(threads)])\n\n    for attempt in range(1, retries + 1):\n        try:\n            result = subprocess.run(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                text=True,\n                check=False,\n                timeout=timeout\n            )\n\n            if stop_spinner:\n                stop_spinner()  # stop the spinner\n\n            if result.returncode != 0:\n                print(f\"[Error] llama-cli exited with code {result.returncode}. Retrying ({attempt}/{retries})...\")\n                time.sleep(1)\n                continue\n\n            output = result.stdout\n            with open(TEMP_OUTPUT, 'w', encoding='utf-8') as f:\n                f.write(output)\n            return output\n\n        except subprocess.TimeoutExpired:\n            if stop_spinner: stop_spinner()\n            print(f\"[Warning] llama-cli timed out (attempt {attempt}/{retries}). Retrying...\")\n        except Exception as e:\n            if stop_spinner: stop_spinner()\n            print(f\"[Error] Unexpected exception: {e}\")\n            time.sleep(1)\n\n    print(f\"[Error] Failed after {retries} attempts.\")\n    sys.exit(1)\n\n\n\ndef extract_response():\n    try:\n        with open(TEMP_OUTPUT, 'r', encoding='utf-8') as f:\n            output = f.read()\n    except FileNotFoundError:\n        return f\"[Error: '{TEMP_OUTPUT}' not found.]\"\n\n    start_marker = '<|start_header_id|>assistant<|end_header_id|>'\n    try:\n        start_pos = output.rindex(start_marker) + len(start_marker)\n        response_block = output[start_pos:]\n    except ValueError:\n        start_marker_alt = '\\nassistant\\n'\n        try:\n            start_pos = output.rindex(start_marker_alt) + len(start_marker_alt)\n            response_block = output[start_pos:]\n        except ValueError:\n            return \"[No response extracted - 'assistant' marker missing.]\"\n\n    end_marker = '[end of text]'\n    end_pos = response_block.find(end_marker)\n    clean_response = response_block[:end_pos].strip() if end_pos != -1 else response_block.strip()\n\n    # Robustly find a JSON object OR a JSON list\n    json_obj_start = clean_response.find('{')\n    json_obj_end = clean_response.rfind('}')\n    json_list_start = clean_response.find('[')\n    json_list_end = clean_response.rfind(']')\n\n    if json_obj_start != -1 and json_obj_end != -1:\n        # It's an object\n        return clean_response[json_obj_start : json_obj_end + 1]\n    elif json_list_start != -1 and json_list_end != -1:\n        # It's a list\n        return clean_response[json_list_start : json_list_end + 1]\n\n    return clean_response\n\n\ndef spinner(message=\"Working...\"):\n    stop_flag = {\"done\": False}\n\n    def spin():\n        for c in itertools.cycle(['|', '/', '-', '\\\\']):\n            if stop_flag[\"done\"]:\n                break\n            sys.stdout.write(f'\\r{message} {c}')\n            sys.stdout.flush()\n            time.sleep(0.1)\n        sys.stdout.write('\\r' + ' ' * (len(message) + 4) + '\\r')\n\n    t = threading.Thread(target=spin)\n    t.daemon = True\n    t.start()\n\n    def stop():\n        stop_flag[\"done\"] = True\n        t.join()\n\n    return stop\n\n\nimport threading\nimport itertools\n\ndef spinner(message=\"Thinking...\"):\n    stop_flag = {\"done\": False}\n\n    def spin():\n        for c in itertools.cycle(['|', '/', '-', '\\\\']):\n            if stop_flag[\"done\"]:\n                break\n            sys.stdout.write(f'\\r{message} {c}')\n            sys.stdout.flush()\n            time.sleep(0.1)\n        sys.stdout.write('\\r' + ' ' * (len(message) + 4) + '\\r')\n        sys.stdout.flush()\n\n    t = threading.Thread(target=spin)\n    t.daemon = True\n    t.start()\n\n    def stop():\n        stop_flag[\"done\"] = True\n        t.join()\n\n    return stop\n\n\n#######################################\n# Main\n#######################################\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Chat with local LLM for Cyber analysis',\n        prog='chat',\n        formatter_class=argparse.RawTextHelpFormatter\n    )\n\n    # Argument groups\n    prompt_group = parser.add_argument_group('Prompting Arguments')\n    gen_group = parser.add_argument_group('Generation Arguments')\n    log_group = parser.add_argument_group('Log Management Arguments')\n    perf_group = parser.add_argument_group('Performance Arguments')\n\n    # Prompting\n    prompt_group.add_argument('-p', '--prompt', help='The prompt/question to send to the LLM.')\n    prompt_group.add_argument('-f', '--file', nargs='+', metavar='FILE_PATH', help='Files to read and include in prompt.')\n    \n    # NEW FLAG\n    prompt_group.add_argument('--check-size', action='store_true',\n                              help=f'Check the size of provided files (limit = {MAX_FILE_SIZE} bytes).')\n\n    # Log management\n    parser.add_argument('--view-log', action='store_true', help='View the chat log file')\n    parser.add_argument('--clear-log', action='store_true', help='Clear the chat log file')\n    parser.add_argument('--view-temp', action='store_true', help='View the temporary LLM output file')\n    parser.add_argument('--clear-temp', action='store_true', help='Clear the temporary LLM output file')\n\n    # Generation\n    gen_group.add_argument('--config', metavar='FILE_PATH', help='Load settings from a specified config file.')\n    gen_group.add_argument('--reset-config', action='store_true', help='Reset session configuration.')\n    gen_group.add_argument('--view-config', action='store_true', help='Display active configuration.')\n    gen_group.add_argument('--system', help='Override default system prompt.')\n    gen_group.add_argument('--tokens', type=int, help='Maximum number of tokens to generate.')\n    gen_group.add_argument('--temp', type=float, dest='temperature', help='Randomness of output.')\n    gen_group.add_argument('--top-p', type=float, help='Top-p sampling.')\n    gen_group.add_argument('--repeat-penalty', type=float, help='Repeat penalty.')\n    gen_group.add_argument('-s', '--seed', type=int, help='Random seed for reproducibility.')\n\n    # Performance\n    perf_group.add_argument('--ctx-size', type=int, help='Prompt context window size.')\n    perf_group.add_argument('--threads', type=int, help='Number of CPU threads.')\n    perf_group.add_argument('--full-verbose', action='store_true', help='Show full LLM output.')\n    perf_group.add_argument('--show-work', action='store_true', help='Display the intermediate map and findings after the final answer.')\n\n\n    args = parser.parse_args()\n\n    # --- Configuration Loading ---\n\n    # --- NEW: File size checker ---\n    if args.check_size and args.file:\n        print(\"\\n--- File Size Check ---\")\n        for path in args.file:\n            if not os.path.exists(path):\n                print(f\"[Missing] {path}\")\n                continue\n            size_bytes = os.path.getsize(path)\n            size_kb = size_bytes / 1024\n            status = \"OK\" if size_bytes <= MAX_FILE_SIZE else \"TOO LARGE\"\n            print(f\"{path}: {size_kb:.1f} KB ({size_bytes} bytes) -> {status}\")\n        sys.exit(0)\n\n        \n    if args.reset_config:\n        clear_session_config_path()\n        sys.exit(0)\n\n    settings = {\n        'cli_bin_loc': CLI_BIN_LOC,\n        'model_loc': MODEL_LOC,\n        'system_prompt': SYSTEM_PROMPT,\n        'tokens': 512,\n        'temperature': 0.8,\n        'top_p': 0.9,\n        'repeat_penalty': 1.0,\n        'seed': None,\n        'ctx_size': 2048,\n        'threads': None\n    }\n    session_config_path = get_session_config_path()\n    if session_config_path:\n        settings.update(load_configuration(session_config_path) or {})\n    if args.config and save_session_config_path(args.config):\n        settings.update(load_configuration(args.config) or {})\n\n    # Override with command-line arguments (only those provided by the user)\n    cli_args = {key: value for key, value in vars(args).items() if value is not None}\n    settings.update(cli_args)\n\n\n    # --- Standalone Command Handling ---\n    if args.view_log: view_file(LOG_FILE); sys.exit(0)\n    if args.clear_log: clear_file(LOG_FILE); sys.exit(0)\n    if args.view_temp: view_file(TEMP_OUTPUT); sys.exit(0)\n    if args.clear_temp: clear_file(TEMP_OUTPUT); sys.exit(0)\n    if args.view_config:\n        print(\"--- Active Configuration Settings ---\")\n        active_settings = {k: v for k, v in settings.items() if hasattr(args, k)}\n        max_key_length = max(len(k) for k in active_settings.keys()) if active_settings else 10\n        for k, v in sorted(active_settings.items()):\n            print(f\"{k:<{max_key_length}} : {v}\")\n        print(\"-----------------------------------\")\n        sys.exit(0)\n\n    # --- Main Execution Logic ---\n    if args.prompt:\n        validate_environment_and_inputs(args.file)\n        run_non_interactive_analysis(args, settings)\n    else:\n        # If no prompt and no other standalone command was run, show help.\n        parser.print_help()\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n##############\n# Build stage\n##############\nFROM ubuntu:24.04 AS builder\n\n# Install dependencies\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n    curl \\\n    wget \\\n    git \\\n    cmake \\\n    g++ \\\n    python3 \\\n    python3-pip \\\n    python3-venv \\\n    ca-certificates \\\n    ccache \\\n    build-essential \\\n    lsb-release \\\n    software-properties-common \\\n    gnupg \\\n    clang && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Install uv using the installer\nADD https://astral.sh/uv/install.sh /uv-installer.sh\nRUN sh /uv-installer.sh && rm /uv-installer.sh\nENV PATH=\"/root/.local/bin/:$PATH\"\n\n# Clone BitNet and setup virtual environment\nWORKDIR /BitNet\nRUN git clone --recursive https://github.com/microsoft/BitNet.git .\nRUN uv venv && \\\n    . .venv/bin/activate && \\\n    uv pip install pip && \\\n    uv pip install PyYAML python-dotenv -r requirements.txt --index-strategy unsafe-best-match && \\\n    mkdir -p models/BitNet-b1.58-2B-4T\n\n# Download the model\nADD https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-gguf/resolve/main/ggml-model-i2_s.gguf models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf\n\n# Setup BitNet environment\nRUN ./.venv/bin/python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s\n\n########################\n# Final stage\n########################\nFROM ubuntu:24.04\n\n# Install Python and create user\nRUN apt-get update && \\\n    apt-get install -y python3-venv && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    useradd -m modeluser\n\n# Copy uv from builder\nCOPY --from=builder /root/.local/bin/uv /usr/local/bin/uv\nENV PATH=\"/home/modeluser/.local/bin:$PATH\"\n\n# Copy BitNet (base code + venv) from builder\nCOPY --from=builder --chown=modeluser:modeluser /BitNet/ /home/modeluser/BitNet/\n\n# Copy required shared libraries\nCOPY --from=builder /BitNet/build/3rdparty/llama.cpp/src/libllama.so /usr/lib/\nCOPY --from=builder /BitNet/build/3rdparty/llama.cpp/ggml/src/libggml.so /usr/lib/\n\n# \u2705 Copy ALL files from the Dockerfile's folder into BitNet folder\nCOPY --chown=modeluser:modeluser . /home/modeluser/BitNet/\n\n# Ensure working directory\nWORKDIR /home/modeluser/BitNet\n\n# Create wrapper script for chat\nRUN echo '#!/bin/sh' > /usr/local/bin/chat && \\\n    echo 'exec /home/modeluser/BitNet/.venv/bin/python /home/modeluser/BitNet/chat.py \"$@\"' >> /usr/local/bin/chat && \\\n    chmod +x /usr/local/bin/chat\n\n# Add a welcome banner for the user\nRUN echo '\\\necho \"===============================================\"\\n\\\necho \"Welcome to the BitNet container!\"\\n\\\necho \"Use the chat command to interact with the AI.\"\\n\\\necho \"Examples: chat -p '\\''Hello AI'\\'' , chat -h\"\\n\\\necho \"===============================================\"\\n\\\n' >> /home/modeluser/.bashrc\n\n# Switch to modeluser\nUSER modeluser\n\n# Default entry\nCMD [\"/bin/bash\"]\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n# Chat\n\nThe chat tool allows us to interact with a language model inside of the GitLab runners with no special hardware! It is based on [Microsoft's BitNet Project](https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source) and uses the [BitNet-b1.58-2B-4T](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T) model with special [terenary quantization](https://arxiv.org/abs/2502.11880) to run efficently on CPU.\n\n### :warning: Warning\n\n**This is an experimental tool container.** Usage of this tool container is **at your own risk**. Language models can return **unexpected or offensive content and present various model security and data privacy concerns**.\n\n## Usage\n\n### Build\n\nBuilding the image locally requires the BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf file to be downloaded. The model weights are stored in this projects model registry. The [gitlab-ci.yml](/.gitlab-ci.yml) contains the job `pre-build-get-model` which provides an example of how to retreive the model weights with `curl`. Once you have the model weights, you can build the image with the following command:\n\n```\ndocker build . -t chat\n```\n\n### Run\n\nIf you've build the container locally, you can run the `chat` command with a prompt. For example,\n\n```\ndocker run -it chat:latest chat \"What is a CVE?\"\n```\n\nThe [gitlab-ci.yml](/.gitlab-ci.yml) also contains the job `post-build-test` which provides an example using the container in a CI/CD pipeline.\n\n", "completion": "No obvious issues found"}
{"prompt": "Analyze this Unknown code for vulnerabilities:\n# Test .env configuration\nTEMPERATURE=0.1\nTOKENS=100\nSYSTEM_PROMPT=\"You are a testing assistant. Respond with only the word 'test-env'.\"\n\n", "completion": "No obvious issues found"}
